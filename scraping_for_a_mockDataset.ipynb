{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff3dc91",
   "metadata": {},
   "source": [
    "### Scraping to create a mock text dataset\n",
    "\n",
    "Scraping https://archiveofourown.org/tags/Halo%20(Video%20Games)%20*a*%20Related%20Fandoms/works to create a mock dataset using BeautifulSoup and requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16687869",
   "metadata": {},
   "source": [
    "1. Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import bs4\n",
    "import scrapy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e9be3d",
   "metadata": {},
   "source": [
    "2. Function that scrapes the entire body of text in one url - inside the url of one work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7962cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_oneWork(work_url):\n",
    "    \n",
    "    \"\"\"\n",
    "    A function that scrapes all the content of one single work\n",
    "    input  : 'str' url of the page\n",
    "    returns: list('str', 'str', 'str'...) list  of strings that make up the body of the work\"\"\"\n",
    "    \n",
    "    url = work_url+'?view_full_work=true'    # view the whole work in one page instead of chapter-wise (which is default)\n",
    "    \n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code == 200:              # HTTP ping successful\n",
    "        soup = bs4.BeautifulSoup(resp.text, 'lxml')\n",
    "        complete_text = [t.getText() for t in soup.select('p')]\n",
    "        \n",
    "    else:\n",
    "        print('Bad response :(')\n",
    "        return None\n",
    "        \n",
    "    return complete_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6fe577",
   "metadata": {},
   "source": [
    "3. Function that sorts through all the work links in one page (1 of 142)\n",
    "> ao3 has a format of 20 works appearing in one page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb0ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_allWorks_in_1Pg(page_url, work_title_tag):\n",
    "    \n",
    "    \"\"\"\n",
    "    Hit all the work links in the current page\n",
    "    input  : 'str' url of the current page\"\"\"\n",
    "    \n",
    "    response = requests.get(page_url)\n",
    "    \n",
    "    if response.status_code == 200:                                # if HTTP response successful\n",
    "        \n",
    "        soup = bs4.BeautifulSoup(response.text, 'lxml')            # parse the html\n",
    "        \n",
    "        work_titles  = [t.getText() for t in soup.select(tag)]     # obtain all the text from the work\n",
    "        \n",
    "        all_links    = [t.get(\"href\") for t in soup.select(tag)]   # all links- both works + authors\n",
    "        \n",
    "        # only select work links and not the authors... filter using presence of the substring \"/works\"\n",
    "        work_links   = [\"https://archiveofourown.org\"+link for link in all_links if '/works' in link]\n",
    "        \n",
    "        \n",
    "        for i, link in enumerate(work_links):           # for each of the work links do...\n",
    "            \n",
    "            time.sleep(5)                               # wait for 5 seconds before calling scrape_oneWork()\n",
    "            \n",
    "            body = \"\\n\".join(scrape_oneWork(link))      # join each paragraph str, separated by \"\\n\" into one str\n",
    "            \n",
    "            f = open(\"ao3_halo.txt\", \"a\")               # Open file in append mode to prevent overwriting\n",
    "            f.writelines(body)\n",
    "            f.close()\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print('Bad response :(')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62032f",
   "metadata": {},
   "source": [
    "4. Parent function that navigates every page under our category of interest, beginning at page=1 from the home url it is called with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_pages(home_url):\n",
    "    \n",
    "    \"\"\"\n",
    "    Make a parent function that goes through every single page of the ao3 halo's 2800~ works...\"\"\"\n",
    "    \n",
    "    for i in range(142):                        # totally 142 pages for this category\n",
    "        \n",
    "        print(f\"Page {i} \\n\")\n",
    "        \n",
    "        page = i+1                              # current page\n",
    "        page_url = home_url+f'?page={page}'     # completing the url by appending the current page number\n",
    "        \n",
    "        # Scrape all the works on this page\n",
    "        scrape_allWorks_in_1Pg(page_url, \"div.header.module h4.heading a\")\n",
    "        \n",
    "        \n",
    "# scrape_all_pages(home_url)    # test function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563069ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_url = \"https://archiveofourown.org/tags/Halo%20(Video%20Games)%20*a*%20Related%20Fandoms/works\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    scrape_all_pages(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
